{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcd61d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "import openpyxl\n",
    "import urllib\n",
    "import selfies as sf\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import re\n",
    "import json\n",
    "import openai\n",
    "import os\n",
    "import tiktoken\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def auto_fill(file_path, sheet_name):\n",
    "    \"\"\"\n",
    "    Automatically fills in the blanks for the specified columns in an Excel sheet.\n",
    "    \n",
    "    Parameters:\n",
    "    - file_path: Path to the Excel file.\n",
    "    - sheet_name: Name of the sheet to process.\n",
    "    \n",
    "    Returns:\n",
    "    - Updated DataFrame with filled values.\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    \n",
    "    # Load the data from the specified sheet\n",
    "    data = pd.read_excel(file_path, sheet_name=sheet_name)\n",
    "    \n",
    "    # Columns to fill\n",
    "    columns_to_fill = [\n",
    "        \"Common names\", \"CAS number\", \"SMILES code (SciFinder)\", \n",
    "        \"Canonical SMILES (PubChem)\", \"Input IUPAC name (cactus)\", \"Input SMILES (cactus)\"\n",
    "    ]\n",
    "    \n",
    "    # Forward fill the specified columns\n",
    "    data[columns_to_fill] = data[columns_to_fill].ffill()\n",
    "    \n",
    "    # Save the updated data back to the same Excel file\n",
    "    with pd.ExcelWriter(file_path, engine='openpyxl', mode='a') as writer:\n",
    "        data.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def fetch_pubchem_data(file_path):\n",
    "    # Read the Excel file\n",
    "    df = pd.read_excel(file_path, engine='openpyxl')\n",
    "    \n",
    "    # Create empty lists to store the results\n",
    "    iupac_names = [\"\"] * len(df)\n",
    "    canonical_smiles_list = [\"\"] * len(df)\n",
    "    \n",
    "    def fetch_data_for_smiles(index, smiles):\n",
    "        # Directly assign \"Invalid\" if the input is \"Invalid\"\n",
    "        if smiles == \"Invalid\":\n",
    "            iupac_names[index] = \"Invalid\"\n",
    "            canonical_smiles_list[index] = \"Invalid\"\n",
    "            return\n",
    "    \n",
    "        def make_api_call(smiles_to_use):\n",
    "            response = requests.get(base_url.format(smiles_to_use))\n",
    "            data = response.json()\n",
    "            iupac_name = data[\"PropertyTable\"][\"Properties\"][0].get(\"IUPACName\", \"Unknown\")\n",
    "            canonical_smiles = data[\"PropertyTable\"][\"Properties\"][0].get(\"CanonicalSMILES\", \"Unknown\")\n",
    "            return iupac_name, canonical_smiles\n",
    "    \n",
    "        try:\n",
    "        # First attempt to make the API call\n",
    "            iupac_name, canonical_smiles = make_api_call(smiles)\n",
    "            iupac_names[index] = iupac_name\n",
    "            canonical_smiles_list[index] = canonical_smiles\n",
    "        except Exception as e:\n",
    "            # If there's an error and the smiles string contains a \"#\"\n",
    "            if \"#\" in smiles:\n",
    "                # Replace \"#\" with \"%23\" and retry the API call\n",
    "                corrected_smiles = smiles.replace(\"#\", \"%23\")\n",
    "                print(f\"Retrying for row {index + 1} with corrected SMILES: {corrected_smiles}\")\n",
    "                try:\n",
    "\n",
    "                    iupac_name, canonical_smiles = make_api_call(corrected_smiles)\n",
    "                    iupac_names[index] = iupac_name\n",
    "                    canonical_smiles_list[index] = canonical_smiles\n",
    "                except Exception as e2:\n",
    "                    print(f\"Error processing row {index + 1} with corrected SMILES: {corrected_smiles}. Error: {e2}\")\n",
    "                    iupac_names[index] = \"Error\"\n",
    "                    canonical_smiles_list[index] = \"Error\"\n",
    "            else:\n",
    "                print(f\"Error processing row {index + 1} with SMILES: {smiles}. Error: {e}\")\n",
    "                iupac_names[index] = \"Error\"\n",
    "                canonical_smiles_list[index] = \"Error\"\n",
    "\n",
    "    \n",
    "    # Base URL for PubChem API\n",
    "    base_url = \"https://pubchem.ncbi.nlm.nih.gov/rest/pug/compound/smiles/{}/property/IUPACName,CanonicalSMILES/JSON\"\n",
    "    \n",
    "    # Process each row in the \"Output cactus SMILES\" column for the first time\n",
    "    for index, smiles in enumerate(df[\"Output SMILES\"]):\n",
    "        print(f\"Processing row {index + 1} with SMILES: {smiles}\")  # Print progress\n",
    "        fetch_data_for_smiles(index, smiles)\n",
    "    \n",
    "    # Retry for errors\n",
    "    for _ in range(2):  # Two more attempts\n",
    "        error_indices = [i for i, name in enumerate(iupac_names) if name == \"Error\"]\n",
    "        if not error_indices:\n",
    "            break  # No errors, break out of the loop\n",
    "\n",
    "        prev_error_count = len(error_indices)\n",
    "        time.sleep(30)\n",
    "        for index in error_indices:\n",
    "            smiles = df[\"Output SMILES\"].iloc[index]\n",
    "            fetch_data_for_smiles(index, smiles)\n",
    "            time.sleep(2)\n",
    "        # Check if the number of errors has decreased\n",
    "        current_error_count = len([i for i, name in enumerate(iupac_names) if name == \"Error\"])\n",
    "        if current_error_count >= prev_error_count:\n",
    "            break  # No improvement, break out of the loop\n",
    "\n",
    "    # Check for remaining errors\n",
    "    final_error_indices = [i for i, name in enumerate(iupac_names) if name == \"Error\"]\n",
    "    if final_error_indices:\n",
    "        error_rows = \", \".join(str(i+1) for i in final_error_indices)\n",
    "        print(f\"\\nCompleted for {file_path}, but rows {error_rows} have errors and need to be verified. Please correct the errors and copy and paste the columns to the main Sheet1 from sheet pubchem output before moving to the next step.\")\n",
    "    \n",
    "    # Assign the results to the dataframe\n",
    "    df[\"Output IUPAC name\"] = iupac_names\n",
    "    df[\"Output Canonical SMILES\"] = canonical_smiles_list\n",
    "    \n",
    "    # Save the updated dataframe to a new sheet named \"output\" in the same Excel file\n",
    "    with pd.ExcelWriter(file_path, engine='openpyxl', mode='a') as writer:\n",
    "        df.to_excel(writer, sheet_name=\"pubchem output\", index=False)\n",
    "\n",
    "        \n",
    "def request_castus_SMILES(file_path, base_url=\"https://cactus.nci.nih.gov/chemical/structure/\"):\n",
    "    df = pd.read_excel(file_path, engine='openpyxl')\n",
    "    \n",
    "    total_rows = len(df)\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        # Skip if \"Output cactus SMILES\" is already filled\n",
    "        if pd.notna(row[\"Output cactus SMILES\"]):\n",
    "            continue\n",
    "\n",
    "        # If \"Output Canonical SMILES\" or \"Output IUPAC name\" or input is \"Invalid\", set \"Output cactus SMILES\" to \"Invalid\"\n",
    "        if row[\"Output Canonical SMILES\"] == \"Invalid\" or row[\"Output IUPAC name\"] == \"Invalid\" or row['Output SMILES'] ==\"Invalid\":\n",
    "            df.at[idx, 'Output cactus SMILES'] = \"Invalid\"\n",
    "            continue\n",
    "\n",
    "        print(f\"Processing row {idx + 1} of {total_rows}...\")\n",
    "\n",
    "        # First, try fetching with \"Output Canonical SMILES\"\n",
    "        response = requests.get(base_url + urllib.parse.quote(row['Output SMILES']) + \"/SMILES\")\n",
    "\n",
    "        # If that fails, try \"Output IUPAC name\"\n",
    "       # if response.status_code == 404:\n",
    "         #   response = requests.get(base_url + urllib.parse.quote(row['Output IUPAC name']) + \"/SMILES\")\n",
    "\n",
    "        # If the response code is 200 (OK), update the \"Output cactus SMILES\" column\n",
    "        if response.status_code == 200:\n",
    "            df.at[idx, 'Output cactus SMILES'] = response.text\n",
    "            #print(f\"SMILES code {response.text} was found using {row['Output SMILES'] if response.text in row['Output SMILES'] else row['Output IUPAC name']}\")\n",
    "        else:\n",
    "            df.at[idx, 'Output cactus SMILES'] = \"Invalid\"\n",
    "            if not (row['Output SMILES'] == \"Invalid\"):\n",
    "                print(f\"SMILES code {urllib.parse.quote(row['Output SMILES'])} was not found.\") \n",
    "\n",
    "    # Save the final DataFrame to a new Excel file\n",
    "    with pd.ExcelWriter(file_path, engine='openpyxl', mode='a') as writer:\n",
    "        df.to_excel(writer, sheet_name=\"castus output\", index=False)\n",
    "    print(f\"Data saved to the 'castus output' sheet in {file_path} for excel {file_path}. Please check if there are any errors and copy and paste the columns to the main Sheet1 from sheet 'castus output' before moving to the next step.\")\n",
    "    \n",
    "def smiles_to_selfies(file_path):\n",
    "    # Read the Excel file\n",
    "    df = pd.read_excel(file_path, engine='openpyxl')\n",
    "\n",
    "    # Process \"Input SMILES (cactus)\" column\n",
    "    for idx, row in df.iterrows():\n",
    "        smiles_value = row[\"Input SMILES (cactus)\"]\n",
    "        if smiles_value == \"Invalid\":\n",
    "            df.at[idx, \"Input SELFIES\"] = \"Invalid\"\n",
    "        else:\n",
    "            try:\n",
    "                selfies_value = sf.encoder(smiles_value)\n",
    "                df.at[idx, \"Input SELFIES\"] = selfies_value\n",
    "            except sf.EncoderError:\n",
    "                print(f\"Error encoding input SMILES at row {idx + 1}: {smiles_value}\")\n",
    "                df.at[idx, \"Input SELFIES\"] = \"Invalid\"\n",
    "\n",
    "    # Process \"Output cactus SMILES\" column\n",
    "    for idx, row in df.iterrows():\n",
    "        smiles_value = row[\"Output cactus SMILES\"]\n",
    "        if smiles_value == \"Invalid\":\n",
    "            df.at[idx, \"Output SELFIES\"] = \"Invalid\"\n",
    "        else:\n",
    "            try:\n",
    "                selfies_value = sf.encoder(smiles_value)\n",
    "                df.at[idx, \"Output SELFIES\"] = selfies_value\n",
    "            except sf.EncoderError:\n",
    "                print(f\"Error encoding output SMILES at row {idx + 1}: {smiles_value}\")\n",
    "                df.at[idx, \"Output SELFIES\"] = \"Invalid\"\n",
    "\n",
    "\n",
    "    with pd.ExcelWriter(file_path, engine='openpyxl',mode='a') as writer:\n",
    "        df.to_excel(writer, sheet_name=\"SELFIES output\", index=False)\n",
    "\n",
    "    print(f\"Updated data saved to {file_path}. Please check if there are any errors and copy and paste the columns to the main Sheet1 from sheet 'SELFIES output' before moving to the next step. \")\n",
    "\n",
    "            \n",
    "def smiles_to_iupac_name(KeyWord):\n",
    "    \n",
    "    if KeyWord ==\"Invalid\" or \"\":\n",
    "        return \"Invalid\"\n",
    "    \n",
    "    # Initialize Chrome browser\n",
    "    browser = webdriver.Chrome()\n",
    "    \n",
    "    # Navigate to the specified URL\n",
    "    browser.get(\"https://app.syntelly.com/smiles2iupac\")\n",
    "    \n",
    "    # Find the input element on the webpage\n",
    "    input_1 = browser.find_element(By.CSS_SELECTOR, 'input[aria-invalid=\"false\"]')\n",
    "    \n",
    "    # Send the keyword to the input element and press Enter\n",
    "    input_1.send_keys(KeyWord)\n",
    "    input_1.send_keys(\"\\n\")\n",
    "\n",
    "    # Wait for the page to load the results\n",
    "    try:\n",
    "        wait = WebDriverWait(browser, 20)\n",
    "        #div_element = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, 'div.sc-gQSkpc.eWIKTX')))\n",
    "        div_element = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, 'div.sc-hybRYi.jJhBHv')))\n",
    "    except:\n",
    "        print(\"Time out, return Unknown\")\n",
    "        return \"Unknown\"\n",
    "\n",
    "    # Get the HTML content of the page\n",
    "    html = browser.page_source\n",
    "    try:\n",
    "        # Use regex to find the string that starts with \"Results: \" and ends with \"</h4>\"\n",
    "        FindStr = re.compile(r'Results: (.*?)</h4>')\n",
    "        result = re.findall(FindStr, html)[0]\n",
    "\n",
    "        # If the result is a space, return \"Invalid\"\n",
    "        if result == \" \" or result == \"\":\n",
    "            return \"Unknown\"\n",
    "\n",
    "        # Find the position of the first comma in the result\n",
    "        comma_index = result.find(', ')\n",
    "        if comma_index != -1:\n",
    "            # If a comma exists, truncate the string up to the first comma\n",
    "            truncated_result = result[:comma_index]\n",
    "        else:\n",
    "            # If no comma exists, return the entire result string\n",
    "            truncated_result = result\n",
    "        return truncated_result\n",
    "    except:\n",
    "        return \"Unknown\"\n",
    "def generate_iupac_names(filename):\n",
    "    # Load the data from the given filename\n",
    "    df = pd.read_excel(filename)\n",
    "    \n",
    "    # Create a new column for IUPAC names if it doesn't exist\n",
    "    if 'Output IUPAC name' not in df.columns:\n",
    "        df['Output IUPAC name'] = \"\"\n",
    "    \n",
    "    # Counters\n",
    "    total_updates = 0\n",
    "    blanks_filled = 0\n",
    "    unknown_or_invalid_updated = 0\n",
    "    \n",
    "    for i, smiles in enumerate(df['Output cactus SMILES']):\n",
    "        # Check if the value is neither empty nor \"Invalid\"\n",
    "        if pd.notna(smiles) and smiles != \"Invalid\":\n",
    "            # Check if the 'Output IUPAC name' cell is empty, \"Unknown\", or \"Error\"\n",
    "            current_iupac_name = df.loc[i, 'Output IUPAC name']\n",
    "            if pd.isna(current_iupac_name) or current_iupac_name in [\"Unknown\", \"Error\"]:\n",
    "                try:\n",
    "                    # Generate the IUPAC name using the provided function\n",
    "                    iupac_name = smiles_to_iupac_name(smiles)\n",
    "                    # Assign the generated IUPAC name to the \"Output IUPAC name\" column in the same row\n",
    "                    df.loc[i, 'Output IUPAC name'] = iupac_name\n",
    "                    \n",
    "                    # Update counters\n",
    "                    total_updates += 1\n",
    "                    if pd.isna(current_iupac_name):\n",
    "                        blanks_filled += 1\n",
    "                        print(f\"Row {i+1}: Filled blank with {iupac_name}\")\n",
    "                    elif current_iupac_name in [\"Unknown\", \"Error\"]:\n",
    "                        unknown_or_invalid_updated += 1\n",
    "                        print(f\"Row {i+1}: Updated '{current_iupac_name}' to {iupac_name}\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Exception at row {i}: {e}\")\n",
    "\n",
    "    # Save the modified dataframe to a new sheet called \"IUPAC output\" in the same Excel file\n",
    "    with pd.ExcelWriter(filename, engine='openpyxl', mode='a') as writer:\n",
    "        df.to_excel(writer, sheet_name='IUPAC output', index=False)\n",
    "\n",
    "    # Print summary\n",
    "    print(f\"\\nSummary:\")\n",
    "    print(f\"Total updates made: {total_updates}\")\n",
    "    print(f\"Number of blanks filled: {blanks_filled}\")\n",
    "    print(f\"Number of 'Unknown' or 'Invalid' entries updated: {unknown_or_invalid_updated}\")\n",
    "    print(f\"Data saved to the 'IUPAC output' sheet in {filename} for {file_path}. Please check if there are any errors and copy and paste the columns to the main Sheet1 from sheet 'IUPAC output' before moving to the next step.\")\n",
    "    \n",
    "\n",
    "def double_check(file_path):\n",
    "    # Read the Excel file into a pandas DataFrame using the provided file path.\n",
    "    df = pd.read_excel(file_path, engine='openpyxl')\n",
    "    \n",
    "    # Define the columns to check, the corresponding base URLs, endpoints, and any values to skip.\n",
    "    columns_to_check = [\n",
    "        (\"Input SMILES (cactus)\", \"https://cactus.nci.nih.gov/chemical/structure/\", \"/SMILES\", []),\n",
    "        (\"Input IUPAC name (cactus)\", \"https://cactus.nci.nih.gov/chemical/structure/\", \"/iupac_name\", []),\n",
    "        (\"Output IUPAC name\", \"https://cactus.nci.nih.gov/chemical/structure/\", \"/iupac_name\", [\"Unknown\", \"Invalid\"]),\n",
    "        (\"Output cactus SMILES\", \"https://cactus.nci.nih.gov/chemical/structure/\", \"/SMILES\", [\"Unknown\", \"Invalid\"])\n",
    "    ]\n",
    "    \n",
    "    # Initialize a counter for the number of values checked.\n",
    "    checked_count = 0\n",
    "    \n",
    "    # Iterate over the columns to check.\n",
    "    for column_name, base_url, endpoint, skip_values in columns_to_check:\n",
    "        previous_value = None  # Keep track of the previous value to identify duplicates.\n",
    "        \n",
    "        # For each column, iterate over each row in the DataFrame.\n",
    "        for idx, row in df.iterrows():\n",
    "            input_value = row[column_name]\n",
    "            \n",
    "            # If the input value is the same as the previous row, skip checking.\n",
    "            if input_value == previous_value:\n",
    "                continue\n",
    "            \n",
    "            # Update the previous value tracker.\n",
    "            previous_value = input_value\n",
    "            \n",
    "            # Additional check for \"Output IUPAC name\" based on \"Output Canonical SMILES\" column.\n",
    "            if column_name == \"Output IUPAC name\" and (row[\"Output Canonical SMILES\"] in [\"Unknown\", \"Invalid\"]):\n",
    "                continue\n",
    "            \n",
    "            # Check if the value in the current row and column is NaN or if it's in the list of values to be skipped.\n",
    "            if pd.isna(input_value) or (skip_values and input_value in skip_values):\n",
    "                continue\n",
    "\n",
    "            # Replace any '#' characters with '%23' for URL encoding.\n",
    "            url_value = input_value.replace('#', '%23')\n",
    "\n",
    "            # Make an HTTP request to the corresponding URL to validate the url_value.\n",
    "            try: \n",
    "                response = requests.get(base_url + url_value + endpoint)\n",
    "                # If the response is more than 100 words, truncate it.\n",
    "                truncated_response = ' '.join(response.text.split()[:100])\n",
    "                # Check conditions for printing error messages\n",
    "                if \"IUPAC\" in column_name and response.status_code == 404:\n",
    "                    # Skip printing for IUPAC names with 404 response\n",
    "                    pass\n",
    "                elif response.status_code != 200 or truncated_response.lower() != input_value.lower():\n",
    "                    print(f\"Error in row {idx + 2} with CAS number {row['CAS number']}. Input: {input_value}, Response: {truncated_response}\")\n",
    "            except:\n",
    "                \n",
    "                print(f\"ConnectionError happens in row {idx + 2} for {url_value}\")\n",
    "\n",
    "            # Increment the counter since a value was checked.\n",
    "            checked_count += 1\n",
    "            # Check if the current count is a multiple of 100\n",
    "            if checked_count % 100 == 0:\n",
    "                print(f\"Checked {checked_count} values so far.\")\n",
    "\n",
    "    # Print the final number of values checked.\n",
    "    print(f\"{checked_count} values have been checked for Sheet1 in {file_path} and done.\")\n",
    "    return\n",
    "\n",
    "\n",
    "def generate_training_data(file_path):\n",
    "\n",
    "\n",
    "    if \"Method \" in file_path and file_path.endswith(\".xlsx\"):\n",
    "        method_type = file_path.split(\"Method \")[1].split(\".\")[0]\n",
    "        if len(method_type) == 1:  # Check if the method_type is a single character\n",
    "            print(method_type)\n",
    "        else:\n",
    "            print(\"Error: Invalid method type format!\")\n",
    "    else:\n",
    "        print(\"Error: Invalid file_path format!\")\n",
    "\n",
    "    method_type = file_path.split(\" \")[1][0]\n",
    "    # Define the action maps for each method type\n",
    "    action_maps = {\n",
    "        \"S\": {\n",
    "            1: \"Introduce or remove a methyl group from the ring.\",\n",
    "            2: \"Introduce or remove a hydroxyl group from the ring.\",\n",
    "            3: \"Introduce or remove an amino group from the ring.\",\n",
    "            4: \"Introduce or remove a nitro group from the ring.\",\n",
    "            5: \"Introduce or remove a fluoro group to the ring.\"\n",
    "        },\n",
    "        \"I\": {\n",
    "            1: \"Insert or remove an unsubstituted phenyl ring at the connection where the carboxylate group is directly attached to either a ring, C=C, C#C, or N=N, ensuring para-positioning.\",\n",
    "            2: \"Insert or remove two carbons along with a triple bond at the connection where the carboxylate group is directly attached to either a ring, C=C, C#C, or N=N.\",\n",
    "            3: \"Insert or remove two carbons along with a double bond at the connection where the carboxylate group is directly attached to either a ring, C=C, C#C, or N=N.\",\n",
    "            4: \"Insert or remove an azo group (-N=N-) at the connection where the carboxylate group is directly attached to either a ring, C=C, C#C, or N=N.\"\n",
    "        },\n",
    "        \"R\": {\n",
    "            1: \"Replace a carbon atom in the ring with nitrogen, or vice versa.\",\n",
    "            2: \"Replace a carbon atom in the ring with oxygen, or vice versa.\",\n",
    "            3: \"Replace a carbon atom in the ring with sulfur, or vice versa.\"\n",
    "        },\n",
    "        \"P\": {\n",
    "            1: \"Shift the position of a COOH group within any ring type to another position on the same ring.\",\n",
    "            2: \"Relocate the position of N donor, excluding NH, within any ring type to another position on the same ring.\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Select the action map based on the method type\n",
    "    action_map = action_maps[method_type]\n",
    "\n",
    "    # Construct the mutation actions section of the prompt\n",
    "    mutation_actions = \"\\n\".join([f\"({key}) {value}\" for key, value in action_map.items()])\n",
    "\n",
    "    # Define the objectives and mutation issues for each method type\n",
    "    objectives = {\n",
    "        \"S\": \"introduce new functional groups or alter existing ones to the linker, then provide the correct molecular representation for the modified linker\",\n",
    "        \"I\": \"insert or delete a linker expansion spacer like phenyl ring, double bond, triple bond, or azo group specifically at the location where a carboxylate group is directly connected to either a ring, a C=C double bond, a C#C triple bond, or an N=N azo group within the linker\",\n",
    "        \"R\": \"swap out atoms in the linker with different heteroatoms (e.g., replace a carbon atom with a nitrogen or sulfur atom), while adhering to general chemical rules and bonding constraints, such as ensuring ring stability and proper valence for atoms, then provide the correct correct molecular representation for the modified linker\",\n",
    "        \"P\": \"change the position of coordination sites, such as COOH or N, within aromatic or non-aromatic rings including 5-membered, 6-membered, 7-membered, and fused rings, then provide the correct correct molecular representation for the modified linker\"\n",
    "    }\n",
    "\n",
    "    mutation_issues = {\n",
    "        \"S\": \"(e.g., it lacks a ring or a suitable substitution site)\",\n",
    "        \"I\": \"(e.g., it lacks a ring or a suitable insertion site between carboxylate and qualified qualified structural groups mentioned above)\",\n",
    "        \"R\": \"(e.g., it lacks a ring or a suitable substitution site)\",\n",
    "        \"P\": \"(e.g., it lacks a ring or a suitable position for the coordination site shift)\"\n",
    "    }\n",
    "\n",
    "    # Define the common prompt with placeholders\n",
    "    common_prompt = f\"\"\"You are an AI assistant with expertise in organic chemistry. Your task is to make theoretical modifications to a given {{desc}} of a MOF linker. {{additional_info}} Your objective is {objectives[method_type]}. You should never remove or modify the carboxylate groups, as they are essential to MOF linkers. The user can choose from {len(action_map)} mutation actions:\n",
    "\n",
    "    {mutation_actions}\n",
    "\n",
    "    The user will first specify the desired mutation action, followed by 'Action: '. In the next line, the user will provide the {{type}} of the MOF linker to be mutated, starting with 'Compound: '.\n",
    "\n",
    "    Your response should begin with 'New Compound: ', followed by the updated {{type}}. If the requested mutation isn't chemically feasible, due to bonding constraints or if the given structure isn't compatible with the mutation {mutation_issues[method_type]}, you should respond with 'New Compound: Invalid'.\"\"\"\n",
    "\n",
    "    # Specific parts for each type of data\n",
    "    prompts = {\n",
    "        'smiles': common_prompt.format(\n",
    "            desc='SMILES code', \n",
    "            additional_info='', \n",
    "            type='SMILES code'\n",
    "        ),\n",
    "        'selfies': common_prompt.format(\n",
    "            desc='SELFIES string', \n",
    "            additional_info='Here SELFIES (SELF-referencIng Embedded Strings) is a string-based representation of molecules. Every SELFIES string corresponds to a valid molecule, similar to the way Canonical SMILES representations work. ',\n",
    "            type='SELFIES string'\n",
    "        ),\n",
    "        'iupac': common_prompt.format(\n",
    "            desc='IUPAC name', \n",
    "            additional_info='', \n",
    "            type='IUPAC name'\n",
    "        )\n",
    "    }\n",
    "\n",
    "\n",
    "    # Function to construct the user message\n",
    "    def construct_user_message(row, data_type):\n",
    "\n",
    "    \n",
    "        input_columns_map = {\n",
    "            'smiles': 'Input SMILES (cactus)',\n",
    "            'canonical smiles': 'Input Canonical SMILES (PubChem)',\n",
    "            'selfies': 'Input SELFIES',\n",
    "            'iupac': 'Input IUPAC name (cactus)'\n",
    "        }\n",
    "    \n",
    "        action = action_map.get(row[\"Action\"], \"Invalid Action\")\n",
    "        return f\"Action: {action}\\nCompound: {row[input_columns_map[data_type]]}\"\n",
    "\n",
    "    def construct_assistant_message(row, data_type):\n",
    "        output_columns_map = {\n",
    "            'smiles': 'Output cactus SMILES',\n",
    "            'canonical smiles': 'Output Canonical SMILES',\n",
    "            'selfies': 'Output SELFIES',\n",
    "            'iupac': 'Output IUPAC name'\n",
    "        }\n",
    "    \n",
    "        return f\"New Compound: {row[output_columns_map[data_type]]}\"\n",
    "\n",
    "    # 1. Load the data from the provided Excel file\n",
    "    data = pd.read_excel(file_path)\n",
    "\n",
    "    # For model using SMILES code\n",
    "    data[\"Modified User Message SMILES\"] = data.apply(lambda row: construct_user_message(row, 'smiles'), axis=1)\n",
    "    data[\"Modified Assistant Message SMILES\"] = data.apply(lambda row: construct_assistant_message(row, 'smiles'), axis=1)\n",
    "\n",
    "    # For model using SMILES code (the canonical smiles choice is optional)\n",
    "    #data[\"Modified User Message Canonical SMILES\"] = data.apply(lambda row: construct_user_message(row, 'canonical smiles'), axis=1)\n",
    "    #data[\"Modified Assistant Message Canonical SMILES\"] = data.apply(lambda row: construct_assistant_message(row, 'canonical smiles'), axis=1)\n",
    "\n",
    "    # For model using SELFIES string\n",
    "    data[\"Modified User Message SELFIES\"] = data.apply(lambda row: construct_user_message(row, 'selfies'), axis=1)\n",
    "    data[\"Modified Assistant Message SELFIES\"] = data.apply(lambda row: construct_assistant_message(row, 'selfies'), axis=1)\n",
    "\n",
    "    # For model using IUPAC name\n",
    "    data[\"Modified User Message IUPAC\"] = data.apply(lambda row: construct_user_message(row, 'iupac'), axis=1)\n",
    "    data[\"Modified Assistant Message IUPAC\"] = data.apply(lambda row: construct_assistant_message(row, 'iupac'), axis=1)\n",
    "\n",
    "\n",
    "\n",
    "    # 2. Model 1R - Using SMILES code\n",
    "    model_1R = pd.DataFrame({\n",
    "        \"system\": [prompts['smiles']] * len(data),\n",
    "        \"user\": data[\"Modified User Message SMILES\"],\n",
    "        \"assistant\": data[\"Modified Assistant Message SMILES\"]\n",
    "    })\n",
    "    model_1R.to_excel(\"Model 1\"+method_type+\".xlsx\", index=False)\n",
    "\n",
    "\n",
    "    # 3. Model 2R - Using SELFIES string\n",
    "    model_2R = pd.DataFrame({\n",
    "        \"system\": [prompts['selfies']] * len(data),\n",
    "        \"user\": data[\"Modified User Message SELFIES\"],\n",
    "        \"assistant\": data[\"Modified Assistant Message SELFIES\"]\n",
    "    })\n",
    "    model_2R.to_excel(\"Model 2\"+method_type+\".xlsx\", index=False)\n",
    "\n",
    "\n",
    "\n",
    "    # 4. Model 3R - Using IUPAC name with filter\n",
    "    filtered_data_3R = data[data[\"Output IUPAC name\"] != \"Unknown\"]\n",
    "    model_3R = pd.DataFrame({\n",
    "        \"system\": [prompts['iupac']] * len(filtered_data_3R),\n",
    "        \"user\": filtered_data_3R[\"Modified User Message IUPAC\"],\n",
    "        \"assistant\": filtered_data_3R[\"Modified Assistant Message IUPAC\"]\n",
    "    })\n",
    "    model_3R.to_excel(\"Model 3\"+method_type+\".xlsx\", index=False)\n",
    "\n",
    "    \n",
    "    print (\"Training data for Model 1 2 3 for Method \" + method_type + \" generated.\")\n",
    "    \n",
    "    return\n",
    "\n",
    "def generate_json_from_excel(model):\n",
    "    #e.g. model= \"2R\"\n",
    "    # Read the .xlsx file, ensuring 'N/A' is treated as a string\n",
    "    data = pd.read_excel(\"Model \"+model+\".xlsx\", na_values=[], keep_default_na=False)\n",
    "\n",
    "    # Define the path for the output .jsonl file\n",
    "    output_path = \"Model \"+model+\"_json.jsonl\"\n",
    "\n",
    "    # Open the file in write mode\n",
    "    with open(output_path, 'w') as file:\n",
    "        # Iterate over each row in the dataframe\n",
    "        for index, row in data.iterrows():\n",
    "            # Create the JSON object for each row\n",
    "            json_obj = {\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"system\", \"content\": row[\"system\"]},\n",
    "                    {\"role\": \"user\", \"content\": row[\"user\"]},\n",
    "                    {\"role\": \"assistant\", \"content\": row[\"assistant\"]}\n",
    "                ]\n",
    "            }\n",
    "            # Write the JSON object to the file\n",
    "            file.write(json.dumps(json_obj) + '\\n')\n",
    "\n",
    "    print(f\"Data of Model {model} has been successfully converted and saved to {output_path}\")\n",
    "    return output_path\n",
    "\n",
    "    \n",
    "def check_json(model):\n",
    "    #function provide by OpenAI\n",
    "    #we specify the data path and open the JSONL file\n",
    "\n",
    "    data_path = \"Model \"+model+\"_json.jsonl\"\n",
    "\n",
    "    # Load dataset\n",
    "    with open(data_path) as f:\n",
    "        dataset = [json.loads(line) for line in f]\n",
    "\n",
    "    # We can inspect the data quickly by checking the number of examples and the first item\n",
    "\n",
    "    # Initial dataset stats\n",
    "    print(\"Num examples:\", len(dataset))\n",
    "    print(\"First example:\")\n",
    "    for message in dataset[0][\"messages\"]:\n",
    "        print(message)\n",
    "\n",
    "    # Now that we have a sense of the data, we need to go through all the different examples and check to make sure the formatting is correct and matches the Chat completions message structure\n",
    "\n",
    "    # Format error checks\n",
    "    format_errors = defaultdict(int)\n",
    "\n",
    "    for ex in dataset:\n",
    "        if not isinstance(ex, dict):\n",
    "            format_errors[\"data_type\"] += 1\n",
    "            continue\n",
    "\n",
    "        messages = ex.get(\"messages\", None)\n",
    "        if not messages:\n",
    "            format_errors[\"missing_messages_list\"] += 1\n",
    "            continue\n",
    "\n",
    "        for message in messages:\n",
    "            if \"role\" not in message or \"content\" not in message:\n",
    "                format_errors[\"message_missing_key\"] += 1\n",
    "\n",
    "            if any(k not in (\"role\", \"content\", \"name\") for k in message):\n",
    "                format_errors[\"message_unrecognized_key\"] += 1\n",
    "\n",
    "            if message.get(\"role\", None) not in (\"system\", \"user\", \"assistant\"):\n",
    "                format_errors[\"unrecognized_role\"] += 1\n",
    "\n",
    "            content = message.get(\"content\", None)\n",
    "            if not content or not isinstance(content, str):\n",
    "                format_errors[\"missing_content\"] += 1\n",
    "\n",
    "        if not any(message.get(\"role\", None) == \"assistant\" for message in messages):\n",
    "            format_errors[\"example_missing_assistant_message\"] += 1\n",
    "\n",
    "    if format_errors:\n",
    "        print(\"Found errors:\")\n",
    "        for k, v in format_errors.items():\n",
    "            print(f\"{k}: {v}\")\n",
    "    else:\n",
    "        num_error=0\n",
    "        print(\"No errors found\")\n",
    "\n",
    "    # Beyond the structure of the message, we also need to ensure that the length does not exceed the 4096 token limit.\n",
    "\n",
    "    # Token counting functions\n",
    "    encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "    # not exact!\n",
    "    # simplified from https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb\n",
    "    def num_tokens_from_messages(messages, tokens_per_message=3, tokens_per_name=1):\n",
    "        num_tokens = 0\n",
    "        for message in messages:\n",
    "            num_tokens += tokens_per_message\n",
    "            for key, value in message.items():\n",
    "                if not isinstance(value, str):\n",
    "                    print(f\"Error in message: {message}\")\n",
    "                    print(f\"Invalid value for key '{key}': {value} (type: {type(value)})\")\n",
    "                    continue\n",
    "                num_tokens += len(encoding.encode(value))\n",
    "                if key == \"name\":\n",
    "                    num_tokens += tokens_per_name\n",
    "        num_tokens += 3\n",
    "        return num_tokens\n",
    "\n",
    "    def num_assistant_tokens_from_messages(messages):\n",
    "        num_tokens = 0\n",
    "        for message in messages:\n",
    "            if message[\"role\"] == \"assistant\":\n",
    "                num_tokens += len(encoding.encode(message[\"content\"]))\n",
    "        return num_tokens\n",
    "\n",
    "    def print_distribution(values, name):\n",
    "        print(f\"\\n#### Distribution of {name}:\")\n",
    "        print(f\"min / max: {min(values)}, {max(values)}\")\n",
    "        print(f\"mean / median: {np.mean(values)}, {np.median(values)}\")\n",
    "        print(f\"p5 / p95: {np.quantile(values, 0.1)}, {np.quantile(values, 0.9)}\")\n",
    "\n",
    "    # Last, we can look at the results of the different formatting operations before proceeding with creating a fine-tuning job:\n",
    "\n",
    "    # Warnings and tokens counts\n",
    "    n_missing_system = 0\n",
    "    n_missing_user = 0\n",
    "    n_messages = []\n",
    "    convo_lens = []\n",
    "    assistant_message_lens = []\n",
    "\n",
    "    for ex in dataset:\n",
    "        messages = ex[\"messages\"]\n",
    "        if not any(message[\"role\"] == \"system\" for message in messages):\n",
    "            n_missing_system += 1\n",
    "        if not any(message[\"role\"] == \"user\" for message in messages):\n",
    "            n_missing_user += 1\n",
    "        n_messages.append(len(messages))\n",
    "        convo_lens.append(num_tokens_from_messages(messages))\n",
    "        assistant_message_lens.append(num_assistant_tokens_from_messages(messages))\n",
    "\n",
    "    print(\"Num examples missing system message:\", n_missing_system)\n",
    "    print(\"Num examples missing user message:\", n_missing_user)\n",
    "    print_distribution(n_messages, \"num_messages_per_example\")\n",
    "    print_distribution(convo_lens, \"num_total_tokens_per_example\")\n",
    "    print_distribution(assistant_message_lens, \"num_assistant_tokens_per_example\")\n",
    "    n_too_long = sum(l > 4096 for l in convo_lens)\n",
    "    print(f\"\\n{n_too_long} examples may be over the 4096 token limit, they will be truncated during fine-tuning\")\n",
    "\n",
    "    # Pricing and default n_epochs estimate\n",
    "    MAX_TOKENS_PER_EXAMPLE = 4096\n",
    "\n",
    "    MIN_TARGET_EXAMPLES = 100\n",
    "    MAX_TARGET_EXAMPLES = 25000\n",
    "    TARGET_EPOCHS = 3\n",
    "    MIN_EPOCHS = 1\n",
    "    MAX_EPOCHS = 25\n",
    "\n",
    "    n_epochs = TARGET_EPOCHS\n",
    "    n_train_examples = len(dataset)\n",
    "    if n_train_examples * TARGET_EPOCHS < MIN_TARGET_EXAMPLES:\n",
    "        n_epochs = min(MAX_EPOCHS, MIN_TARGET_EXAMPLES // n_train_examples)\n",
    "    elif n_train_examples * TARGET_EPOCHS > MAX_TARGET_EXAMPLES:\n",
    "        n_epochs = max(MIN_EPOCHS, MAX_TARGET_EXAMPLES // n_train_examples)\n",
    "\n",
    "    n_billing_tokens_in_dataset = sum(min(MAX_TOKENS_PER_EXAMPLE, length) for length in convo_lens)\n",
    "    print(f\"Dataset has ~{n_billing_tokens_in_dataset} tokens that will be charged for during training\")\n",
    "    print(f\"By default, you'll train for {n_epochs} epochs on this dataset\")\n",
    "    print(f\"By default, you'll be charged for ~{n_epochs * n_billing_tokens_in_dataset} tokens\")    \n",
    "    \n",
    "    return num_error \n",
    "    \n",
    "def start_ft(model):\n",
    "    new_upload = openai.File.create(\n",
    "      file=open( \"Model \"+model+\"_json.jsonl\", \"rb\"),\n",
    "      purpose='fine-tune'\n",
    "    )\n",
    "    print(\"\\n\")\n",
    "    print(f\"Data of Model {model} has been uploaded. Please wait for 2 minutes to start the job.\")\n",
    "    print(new_upload)\n",
    "    time.sleep(120)\n",
    "\n",
    "\n",
    "    for _ in range(100):  # Max attempts\n",
    "        print(f\"Try to create fine-tuning job for Model {model}. The job ID is {new_upload.id}\")\n",
    "        try:\n",
    "            ft_model = openai.FineTuningJob.create(training_file=new_upload.id, model=\"gpt-3.5-turbo\")\n",
    "            print(\"Fine-tuning job created successfully! Please check the email for the model ID.\")\n",
    "            print(ft_model)\n",
    "            print(\"\\n\")\n",
    "            break\n",
    "        except openai.error.InvalidRequestError as e:\n",
    "            if \"still being processed\" in str(e):\n",
    "                print(\"File is still being processed. Retrying in 30 seconds...\")\n",
    "                time.sleep(30)  # Wait for 30 seconds\n",
    "            else:\n",
    "                raise e\n",
    "        except openai.error.RateLimitError as e:\n",
    "            if \"rate-limited\" in str(e):\n",
    "                if \"per day\"in str(e):\n",
    "                    print(\"12 task per day limit reached.\")\n",
    "                    time.sleep(10000)  # Wait for 90 minutes\n",
    "                print(\"Rate limit reached. Retrying in 10 minutes...\")\n",
    "                time.sleep(600)  # Wait for 10 minutes\n",
    "            else:\n",
    "                raise e\n",
    "      \n",
    "    else:\n",
    "        print(\"Max attempts reached. Fine-tuning job could not be created.\")\n",
    "    return new_upload.id    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_ft_responses(model,  ft_model_id, user_messages_sample,action_number=\"all\"):\n",
    "    \"\"\"\n",
    "    Function to get AI responses based on the provided model, action number, and a list of user messages.\n",
    "\n",
    "    Parameters:\n",
    "    - model (str): Model name used to load the corresponding Excel file.\n",
    "    - user_messages_sample (list): List of user messages for testing.\n",
    "    - ft_model_id (str): Fine-tuned model ID to be used for AI completions.\n",
    "    - action_number (str): The specific action number or \"all\". Default is \"all\".\n",
    "\n",
    "    Returns:\n",
    "    - List of AI's responses.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. LOAD THE DATA\n",
    "    # Read the .xlsx file, ensuring 'N/A' is treated as a string\n",
    "    data = pd.read_excel(f\"Model {model}.xlsx\", na_values=[], keep_default_na=False)\n",
    "\n",
    "    # Extract the system message from the first non-header row of the 'system' column\n",
    "    system_message = data['system'].iloc[0]\n",
    "\n",
    "    \n",
    "    # 2. EXTRACT ACTION CHOICES\n",
    "    # Use regular expressions to find all action patterns (like (1), (2), etc.) from the system message\n",
    "    action_patterns = re.findall(r'\\(\\d\\)', system_message)\n",
    "\n",
    "    # Count the number of actions\n",
    "    number_of_actions = len(action_patterns)\n",
    "\n",
    "    # Extract descriptions for each action for later use in user messages\n",
    "    action_descriptions = [re.search(r'\\({}\\) (.+?)\\.\\n'.format(i+1), system_message).group(1) for i in range(number_of_actions)]\n",
    "\n",
    "    # 3. ADJUST USER MESSAGES BASED ON ACTION_NUMBER\n",
    "    adjusted_user_messages = []\n",
    "    if action_number == \"all\":\n",
    "        for compound in user_messages_sample:\n",
    "            for i, action_description in enumerate(action_descriptions):\n",
    "                # Combine action and compound to create a user message\n",
    "                message = \"Action: ({}) {}\\nCompound: {}\".format(i + 1, action_description, compound)\n",
    "                adjusted_user_messages.append(message)\n",
    "    else:\n",
    "        # Validate the action number\n",
    "        if 1 <= int(action_number) <= number_of_actions:\n",
    "            for compound in user_messages_sample:\n",
    "                action_description = action_descriptions[int(action_number) - 1]\n",
    "                message = \"Action: ({}) {}\\nCompound: {}\".format(action_number, action_description, compound)\n",
    "                adjusted_user_messages.append(message)\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid action_number. It should be between 1 and {number_of_actions} or 'all'.\")\n",
    "\n",
    "    # 4. INTERACT WITH THE AI MODEL\n",
    "    # Loop through the adjusted user messages and send them to the AI model\n",
    "    responses = []\n",
    "    for user_message in adjusted_user_messages:\n",
    "  \n",
    "        completion = openai.ChatCompletion.create(\n",
    "            model=ft_model_id,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_message},\n",
    "                {\"role\": \"user\", \"content\": user_message}\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        #print(completion.choices[0].message)\n",
    "        # Append the AI's response to the responses list\n",
    "        responses.append(completion.choices[0].message)\n",
    "    \n",
    "    return responses\n",
    "\n",
    "\n",
    "def evaluation(model_type):\n",
    "    output_column_list = [\"GPT-3.5 (SMILES)\",\n",
    "                      \"GPT-3.5 (IUPAC)\",\n",
    "                      \"GPT-4 (SMILES)\",\n",
    "                      \"GPT-4 (IUPAC)\",\n",
    "                      \"FT Model 1\"+model_type+\" (SMILES)\",\n",
    "                      \"FT Model 2\"+model_type+\" (SELFIES)\",\n",
    "                      \"FT Model 3\"+model_type+\" (IUPAC)\"]\n",
    "                      #\"FT Model 4\"+model_type+\" (SMILES)\",\n",
    "                     # \"FT Model 5\"+model_type+\" (SELFIES)\",\n",
    "                      # \"FT Model 6\"+model_type+\" (IUPAC)\"]  Can include more models if needed\n",
    "\n",
    "    model_name_list = [ \"1\"+model_type, #using same prompt as 1R (SMILES)\n",
    "                   \"3\"+model_type,  #using same prompt as 3R (IUPAC)\n",
    "                   \"1\"+model_type,\n",
    "                   \"3\"+model_type,\n",
    "                   \"1\"+model_type,\n",
    "                   \"2\"+model_type,\n",
    "                   \"3\"+model_type]\n",
    "                   #\"4\"+model_type,\n",
    "                  # \"5\"+model_type,\n",
    "                   #\"6\"+model_type]\n",
    "\n",
    "\n",
    "    file_path =\"Evaluation Medtod \"+model_type+\".xlsx\"\n",
    "\n",
    "    # Read the Excel file\n",
    "    df = pd.read_excel(file_path)\n",
    "\n",
    "    # Initialize an empty DataFrame for the output\n",
    "    output_df = pd.DataFrame(index=df.index)\n",
    "\n",
    "    # Define a dictionary to map output column description to input column names\n",
    "    input_column_map = {\n",
    "        \"(SMILES)\": \"Input SMILES (cactus)\",\n",
    "        \"(IUPAC)\": \"Input IUPAC name (cactus)\",\n",
    "        \"(SELFIES)\": \"Input SELFIES\"\n",
    "    }\n",
    "\n",
    "    # Iterate over each output column\n",
    "    for idx, (output_column, model_name, model_id) in enumerate(zip(output_column_list, model_name_list, model_id_list)):\n",
    "    \n",
    "        print(f\"Processing column {idx+1}/{len(output_column_list)}: {output_column} using system prompt model: {model_name} for model {model_id}\")\n",
    "    \n",
    "        # Determine the input column based on the description in the output column name\n",
    "        for desc, input_col in input_column_map.items():\n",
    "            if desc in output_column:\n",
    "                input_column = input_col\n",
    "                break\n",
    "    \n",
    "        # Adjust user messages based on the action_number column and the input column\n",
    "        user_messages_sample = df[input_column].tolist()\n",
    "        action_numbers = df[\"Action\"].tolist()\n",
    "    \n",
    "        # Collect AI's responses\n",
    "        ai_responses = []\n",
    "        for uidx, (user_message, action_number) in enumerate(zip(user_messages_sample, action_numbers)):\n",
    "            print(f\"\\tRequesting response for message {uidx+1}/{len(user_messages_sample)}\")\n",
    "        \n",
    "            attempts = 0\n",
    "            max_attempts = 5\n",
    "            waiting_time = 10\n",
    "            success = False\n",
    "            while attempts < max_attempts and not success:\n",
    "                try:\n",
    "                    responses = get_ft_responses(model_name, model_id, [user_message], str(action_number))\n",
    "                    # Process the JSON-like responses\n",
    "                    for response_json in responses:\n",
    "                        # Extract the part after \"New Compound:\"\n",
    "                        compound = response_json[\"content\"].split(\"New Compound: \")[-1].strip()\n",
    "                        ai_responses.append(compound)\n",
    "                    success = True\n",
    "                except Exception as e:\n",
    "                    \n",
    "                    attempts += 1\n",
    "                    print(f\"\\tError on attempt {attempts}. Retrying in {waiting_time} seconds...\")\n",
    "                    time.sleep(waiting_time)\n",
    "                    waiting_time = waiting_time*2\n",
    "                    if attempts == max_attempts:\n",
    "                        print(f\"\\tFailed after {max_attempts} attempts. Moving on to the next message.\")\n",
    "                        ai_responses.append(\"Error\")\n",
    "\n",
    "        # Store the AI's responses in the output_df\n",
    "        output_df[output_column] = ai_responses\n",
    "\n",
    "    print(\"Processing complete!\")\n",
    "\n",
    "    # Update the original Excel file with the values from output_df\n",
    "    with pd.ExcelWriter(file_path, engine='openpyxl', mode='a') as writer:\n",
    "        output_df.to_excel(writer, sheet_name='Eva Output', index=False)\n",
    "    \n",
    "    #add two more columns to decode the SELFIES    \n",
    "    # Open the 'Eva Output' sheet from the Excel file\n",
    "    with pd.ExcelFile(file_path) as xls:\n",
    "        eva_output_df = pd.read_excel(xls, 'Eva Output')\n",
    "\n",
    "    # Function to decode the values or return \"Invalid\"\n",
    "    def decode_or_invalid(value):\n",
    "        if value == \"Invalid\":\n",
    "            return \"Invalid\"\n",
    "        try:\n",
    "            return sf.decoder(value)\n",
    "        except:\n",
    "            return \"Invalid\"\n",
    "\n",
    "\n",
    "    # Decode the values in \"FT Model 2R (SELFIES)\" and \"FT Model 5R (SELFIES)\"\n",
    "    # and insert them right after the respective columns\n",
    "    eva_output_df.insert(\n",
    "        eva_output_df.columns.get_loc(\"FT Model 2\"+model_type+\" (SELFIES)\") + 1,\n",
    "        \"FT Model 2\"+model_type+\" (SELFIES_Decode)\",\n",
    "        eva_output_df[\"FT Model 2\"+model_type+\" (SELFIES)\"].apply(decode_or_invalid)\n",
    "    )\n",
    "\n",
    "    #uncomment code below if there are additional models need SELFIES conversion\n",
    "   # eva_output_df.insert(\n",
    "    #    eva_output_df.columns.get_loc(\"FT Model 5\"+model_type+\" (SELFIES)\") + 1,\n",
    "   #     \"FT Model 5\"+model_type+\" (SELFIES_Decode)\",\n",
    "   #     eva_output_df[\"FT Model 5\"+model_type+\" (SELFIES)\"].apply(decode_or_invalid)\n",
    "   # )\n",
    "    \n",
    "\n",
    "    # Save the updated DataFrame to a new sheet named 'Eva Output Decode'\n",
    "    with pd.ExcelWriter(file_path, engine='openpyxl', mode='a') as writer:\n",
    "        eva_output_df.to_excel(writer, sheet_name='Eva Output Decode', index=False)\n",
    "    \n",
    "    return\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23709663",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "83644fe7",
   "metadata": {},
   "source": [
    "# Fetch Chemical Representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0969a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please double check the file path and method chosen!!\n",
    "method_type = \"X\"  # This can be changed to \"S\", \"I\", \"P\" as needed\n",
    "file_path = \"Method \" + method_type + \".xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21321a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch_pubchem_data(file_path)  #this function will check and retrieve the PubChem IUPAC names and Canomical smiles for given compound\n",
    "                               #If the result is \"Unknown\", this compound is likely to be purely hyperthetical and have not been reported "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07980d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "request_castus_SMILES(file_path) #this function will get a valid smiles code in a systematic way\n",
    "                                #for all structures after mutation, regardless of they have been reported or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf041ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "smiles_to_selfies(file_path)   #this function convert all castus smiles code to SELFIES strings and add to the excel table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7304ccdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_iupac_names(file_path)  # this function will go through all compounds that do not have IUPAC names on PubChem\n",
    "                                    #and retrieve their standard IUPAC name using syntelly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6861d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "double_check(file_path)     # this function check all IUPAC names are valid and consistent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bf782c",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_training_data(file_path)  #this function generate a \"system-user-assistant\" prompt dialogue file for every mutation for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f09d1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8aca9af7",
   "metadata": {},
   "source": [
    "# Training GPT Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3baf01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = \"Replace with your real OpenAI API Key\"     #e.g.  \"ab-cdxyzABCkkkQQ2SsAb123DDeeFFggHh\"\n",
    "                                                            #For more information: https://openai.com/blog/openai-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d1db7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 4):  # Loop from Model 1X to Model 4X, where X = S, I, R, P\n",
    "    model = str(i) + method_type\n",
    "    \n",
    "    print(\"\\n++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "    print(f\"Start to process Model {model}.\\n\")\n",
    "    start_time = time.time()  # Start timing\n",
    "    \n",
    "    generate_json_from_excel(model) #first convert the excel file to json \n",
    "    check_json(model)  #using the code provided by OpenAI to check for json before training\n",
    "    start_ft(model)   #upload the json to OpenAI and start fine tuning\n",
    "    \n",
    "    end_time = time.time()  # End timing\n",
    "    elapsed_time = end_time - start_time  # Calculate elapsed time\n",
    "    \n",
    "    print(f\"Finish Model {model}.\\n\")\n",
    "    print(f\"Model {model} took {elapsed_time:.2f} seconds to process.\")\n",
    "    print(\"\\n++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "    \n",
    "# Optional: List 20 fine-tuning jobs\n",
    "#openai.FineTuningJob.list(limit=20)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28348628",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3c1f6d37",
   "metadata": {},
   "source": [
    "# Evaluate FT Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903b6a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id_list = [ \"gpt-3.5-turbo-0613\" , #3.5 SMILES\n",
    "                  \"gpt-3.5-turbo-0613\" , #3.5 IUPAC\n",
    "                    \"gpt-4-0613\" ,#4 SMILES\n",
    "                  \"gpt-4-0613\" ,#4 IUPAC\n",
    "                 \"ft:gpt-3.5-turbo-0613:ADD_Model_ID_Here\",#replace with your model ID\n",
    "               \"ft:gpt-3.5-turbo-0613:ADD_Model_ID_Here\",  #replace with your model ID\n",
    "               \"ft:gpt-3.5-turbo-0613:ADD_Model_ID_HereG\",  #replace with your model ID]\n",
    "\n",
    "evaluation(model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c466b412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below is a list of models trained on Methods S, I, R, and P that are ready for use.\n",
    "# You can copy and paste the model ID into the cell above to run the evaluation.\n",
    "# Ensure you use the exact same system prompt and user input format to guarantee the model functions as expected.\n",
    "# For system prompt and user input samples, please refer to the function \"generate_training_data\" for more details.\n",
    "# Alternatively, you can also obtain the system prompt by reading the \"Model XY.xlsx\" file, where X is a number from 1-3 and Y is either S, I, R, or P.\n",
    "# Depending on the chemical representations (SMILES, SELFIES, IUPAC) and the mutation methods (insertion, replacement, etc.), the system prompt and user message may vary.\n",
    "\n",
    "\n",
    "# model_type =\"R\"\n",
    "model_id_list = [\n",
    "                 \"ft:gpt-3.5-turbo-0613:uc-berkeley::7vd4eEZu\",#model 1R  SMILES\n",
    "               \"ft:gpt-3.5-turbo-0613:uc-berkeley::7veHJ0eR\",  #model 2R  SELFIES\n",
    "               \"ft:gpt-3.5-turbo-0613:uc-berkeley::7vyL332G\",  #model 3R  IUPAC\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#model_type =\"S\"\n",
    "model_id_list = [ \n",
    "                 \"ft:gpt-3.5-turbo-0613:uc-berkeley::7wF4Wvdr\",#model 1S  SMILES\n",
    "               \"ft:gpt-3.5-turbo-0613:uc-berkeley::7wGGcyfU\",  #model 2S  SELFIES\n",
    "               \"ft:gpt-3.5-turbo-0613:uc-berkeley::7wHSe0sw\",  #model 3S  IUPAC\n",
    "\n",
    "]\n",
    "\n",
    "\n",
    "#model_type =\"I\"\n",
    "model_id_list = [ \n",
    "                 \"ft:gpt-3.5-turbo-0613:uc-berkeley::7xJmyNlq\",#model 1I  SMILES\n",
    "               \"ft:gpt-3.5-turbo-0613:uc-berkeley::7xKePzT5\",  #model 2I  SELFIES\n",
    "               \"ft:gpt-3.5-turbo-0613:uc-berkeley::7xM2Vcbv\",  #model 3I  IUPAC\n",
    "\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "#model_type =\"P\"\n",
    "model_id_list = [\n",
    "                 \"ft:gpt-3.5-turbo-0613:uc-berkeley::7xiQHz21\",#model 1P\n",
    "               \"ft:gpt-3.5-turbo-0613:uc-berkeley::7xjKObLF\",  #model 2P\n",
    "               \"ft:gpt-3.5-turbo-0613:uc-berkeley::7xkDldW9\",  #model 3P\n",
    "]\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
